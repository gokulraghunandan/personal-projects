{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2A2Bgokul-batch-6.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gokulraghunandan/personal-projects/blob/master/2A2Bgokul_batch_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "CcvB2_EOuxhJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Assignment 2A#\n",
        "#Backpropogation step by step#\n",
        "![alt text](http://hmkcode.github.io/images/ai/backpropagation.png)\n",
        "\n",
        "If you are building your own neural network, you will definitely need to understand how to train it. Backpropagation is a commonly used technique for training neural network. There are many resources explaining the technique, but this post will explain backpropagation with concrete example in a very detailed colorful steps.\n",
        "\n",
        "##Overview##\n",
        "In this post, we will build a neural network with three layers:\n",
        "* __Input__ layer with two input neurons\n",
        "* One __hidden__ layer with two neurons\n",
        "* __Output__ layer with a single neuron\n",
        "\n",
        "![alt text](http://hmkcode.github.io/images/ai/nn1.png)\n",
        "\n",
        "##Weights,weights, weights##\n",
        "\n",
        "Neural network training is about finding weights that minimize prediction error. We usually start our training with a set of randomly generated weights.Then, backpropagation is used to update the weights in an attempt to correctly map arbitrary inputs to outputs.\n",
        "Our initial weights will be as follows:\n",
        "$W1=0.14, W2=0.24, W3= O.22, W4 = 0.12, W5=0.23, W6=0.17$\n",
        "\n",
        "##Dataset##\n",
        "Our dataset has one sample with two inputs and one output.\n",
        "\n",
        "![alt text](http://hmkcode.github.io/images/ai/bp_dataset.png)\n",
        "\n",
        "Our single sample is as following inputs=[2, 3] and output=[1]\n",
        "\n",
        "![alt text](http://hmkcode.github.io/images/ai/bp_sample.png)\n",
        "\n",
        "##Forward Pass##\n",
        "We will use given weights and inputs to predict the output. Inputs are multiplied by weights; the results are then passed forward to next layer.\n",
        "$$\\begin{pmatrix}2 & 3 \\end{pmatrix} . \\begin{pmatrix} 0.14 & 0.22\\\\\\ 0.24 & 0.12\\end{pmatrix} \\equiv \\begin{pmatrix}1 & 0.8 \\end{pmatrix}.\\begin {pmatrix}0.23\\\\0.17\\end{pmatrix}\\equiv 0.366 $$\n",
        "$2\\times0.14+3\\times0.24 =1$\n",
        "\n",
        "$2\\times 0.22 + 3\\times 0.12= 0.8$\n",
        "\n",
        "$1\\times 0.23 + 0.8 \\times 0.17 = 0.366$\n",
        "\n",
        "\n",
        "##Calculating Error##\n",
        "Now, it’s time to find out how our network performed by calculating the difference between the actual output and predicted one. It’s clear that our network output, or __prediction__, is not even close to __actual output__. We can calculate the difference or the error as following.\n",
        "\n",
        "$Error = \\frac 1 2 (prediction -actual)^2=0.2009$\n",
        "\n",
        "Error is always positive because of square and 1\\2 is added to ease the calculation of derivative.\n",
        "\n",
        "##Reducing Error##\n",
        "Our main goal of the training is to reduce the __error__ or the difference between __prediction__ and __actual output__. Since __actual output__ is constant, “not changing”, the only way to reduce the error is to change prediction value. The question now is, how to change __prediction__ value?\n",
        "\n",
        "By decomposing **prediction** into its basic elements we can find that weights are the variable elements affecting prediction value. In other words, in order to change prediction value, we need to change __weights__ values.\n",
        "\n",
        "![](http://hmkcode.github.io/images/ai/bp_prediction_elements.png)\n",
        "\n",
        "\n",
        "The question now is how to change\\update the weights value so that the error is reduced?\n",
        "The answer is __Backpropagation!__\n",
        "\n",
        "##Backpropogation##\n",
        "Backpropagation, short for “backward propagation of errors”, is a mechanism used to update the weights using gradient descent. It calculates the gradient of the error function with respect to the neural network’s weights. The calculation proceeds backwards through the network.\n",
        "\n",
        "Gradient descent is an iterative optimization algorithm for finding the minimum of a function; in our case we want to minimize th error function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient of the function at the current point.\n",
        "\n",
        "![](http://hmkcode.github.io/images/ai/bp_update_formula.png)\n",
        "\n",
        "For example, to update w6, we take the current w6 and subtract the partial derivative of error function with respect to w6. Optionally, we multiply the derivative of the error function by a selected number to make sure that the new updated weight is minimizing the error function; this number is called learning rate.\n",
        "\n",
        "![](http://hmkcode.github.io/images/ai/bp_w6_update.png)\n",
        "\n",
        "The derivation of the error function is evaluated by applying the chain rule as following\n",
        "![](http://hmkcode.github.io/images/ai/bp_error_function_partial_derivative_w6.png)\n",
        "So to update w6 we can apply the following formula\n",
        "\n",
        "![](http://hmkcode.github.io/images/ai/bp_w6_update_closed_form.png)\n",
        "\n",
        "Similarly, we can derive the update formula for w5 and any other weights existing between the output and the hidden layer.\n",
        "\n",
        "![](http://hmkcode.github.io/images/ai/bp_w5_update_closed_form.png)\n",
        "\n",
        "However, when moving backward to update w1, w2, w3 and w4 existing between input and hidden layer, the partial derivative for the error function with respect to w1, for example, will be as following.\n",
        "\n",
        "![](http://hmkcode.github.io/images/ai/bp_error_function_partial_derivative_w1.png)\n",
        "\n",
        "\n",
        "We can find the update formula for the remaining weights w2, w3 and w4 in the same way.\n",
        "\n",
        "In summary, the update formulas for all weights will be as following:\n",
        "\n",
        "![](http://hmkcode.github.io/images/ai/bp_update_all_weights.png)\n",
        "\n",
        "\n",
        "We can rewrite the update formulas in matrices as following\n",
        "\n",
        "![](http://hmkcode.github.io/images/ai/bp_update_all_weights_matrix.png)\n",
        "\n",
        "\n",
        "##Backward Pass##\n",
        "Using derived formulas we can find the new __weights__.\n",
        "\n",
        "__Learning rate__: is a hyperparameter which means that we need to manually guess its value.\n",
        "\n",
        "$\\Delta=0.366-1=-0.634$\n",
        "\n",
        "$a=0.05$\n",
        "\n",
        "Substituting the values we can calculate:\n",
        "$$\\begin{pmatrix}W5\\\\W6\\end{pmatrix}=\\begin{pmatrix}0.23\\\\0.17\\end{pmatrix}-0.05\\times(-0.634)\\begin{pmatrix}1\\\\0.8\\end{pmatrix}=\\begin{pmatrix}0.23\\\\0.17\\end{pmatrix}-\\begin{pmatrix}-0.0317\\\\-0.0253\\end{pmatrix}=\\begin{pmatrix}0.2617\\\\0.1953\\end{pmatrix}$$\n",
        "\n",
        "\n",
        "$$\\begin{pmatrix}W1 & W3\\\\W2 & W4\\end{pmatrix}=\\begin{pmatrix}0.14 & 0.22\\\\0.24 & 0.12\\end{pmatrix}-0.05(-0.643)\\begin{pmatrix}2\\\\3\\end{pmatrix}.\\begin{pmatrix}0.23&0.77\\end{pmatrix}=\\begin{pmatrix}0.14&0.22\\\\0.24&0.12\\end{pmatrix}-\\begin{pmatrix}-.0146&-.0108\\\\-0.02&-.0161\\end{pmatrix}=\\begin{pmatrix}0.1546&0.2308\\\\0.26&0.1361\\end{pmatrix}$$\n",
        "\n",
        "__Now, using the new weights we will repeat the forward pass__\n",
        "\n",
        "$$\\begin{pmatrix}2&3\\end{pmatrix}.\\begin{pmatrix}0.1546&0.2308\\\\0.26&0.1361\\end{pmatrix}=\\begin{pmatrix}1.08928&0.8699\\end{pmatrix}.\\begin{pmatrix}0.2617\\\\0.1953\\end{pmatrix}=0.4549$$\n",
        ".\n",
        "\n",
        "\n",
        "We can notice that the prediction 0.4549 is a little bit closer to actual output than the previously predicted one 0.366. We can repeat the same process of backward and forward pass until error is close or equal to zero.\n",
        "\n",
        "#Assignment 2B#\n"
      ]
    },
    {
      "metadata": {
        "id": "085WM6xnvdhX",
        "colab_type": "code",
        "outputId": "893eab99-a6e2-4f03-aef1-ff7477d81868",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "  import numpy as np\n",
        "#initialize input variables to 2 and 3\n",
        " \n",
        "  i1=2\n",
        "  i2=3\n",
        "  #input the initial weights\n",
        "  w1=0.14\n",
        "  w2=0.24\n",
        "  w3=0.22\n",
        "  w4=0.12\n",
        "  w5=0.23\n",
        "  w6=0.17\n",
        "  #actual output\n",
        "  aout=1\n",
        "  #input array\n",
        "  I=np.array([i1,i2])\n",
        "  #weight matrix between first two layers\n",
        "  W1=np.array([[w1,w3],[w2,w4]])\n",
        "  #weight matrix between next two layers\n",
        "  W2=np.array([[w5],[w6]])\n",
        " \n",
        "  #forward pass\n",
        "  #calculated output\n",
        "  Z=np.dot(I,W1)\n",
        "  \n",
        "  cout=np.dot(Z,W2)\n",
        "  print(\"Our calculated output is :\")\n",
        "  print(cout)\n",
        "#calculating error\n",
        "  error=(cout-aout)**2\n",
        "  error=error*0.5\n",
        "  print(\"Our error is:\")\n",
        "  print(error)\n",
        "  \n",
        "  #learning rate and delta\n",
        "  a=0.05\n",
        "  delta=(cout-aout)\n",
        "  \n",
        "  IT=np.array([[i1],[i2]])\n",
        "  W2T=np.array([[w5,w6],])\n",
        "  #updated weights\n",
        "  p=(a*delta)*IT\n",
        "  \n",
        "  p=np.dot(p,W2T)\n",
        "  H=np.array([[1],[0.8]])\n",
        "  q=(a*delta)*H\n",
        "  \n",
        "  W1=W1-p\n",
        "  print(\"Our updated W1 matrix for w1-w4 is:\")\n",
        "  print(W1)\n",
        "  W2=W2-q\n",
        "  print(\"Our updated W2 for w5-w6 matrix is\")\n",
        "  print(W2)\n",
        "  #updating the actual weights\n",
        "  w1=0.154582\n",
        "  w2=0.261873\n",
        "  w3=0.230788\n",
        "  w4=0.136167\n",
        "  w5=0.2617\n",
        "  w6=0.19536\n",
        "  \n",
        "  #calculated output with updated weights\n",
        "  Z=np.dot(I,W1)\n",
        "  \n",
        "  cout=np.dot(Z,W2)\n",
        "  print(\"Our new calculated output is :\")\n",
        "  print(cout)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our calculated output is :\n",
            "[0.366]\n",
            "Our error is:\n",
            "[0.200978]\n",
            "Our updated W1 matrix for w1-w4 is:\n",
            "[[0.154582 0.230778]\n",
            " [0.261873 0.136167]]\n",
            "Our updated W2 for w5-w6 matrix is\n",
            "[[0.2617 ]\n",
            " [0.19536]]\n",
            "Our new calculated output is :\n",
            "[0.45647905]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d2JuTncZ5Vb1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}